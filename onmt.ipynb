{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onmt.train_single import main as single_main\n",
    "from onmt.new_opts import Opts, PreprocessorOpts\n",
    "from onmt.preprocess import build_save_dataset, build_save_vocab\n",
    "import onmt.inputters as inputters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_data(data_path, name_to_save, mode):\n",
    "    lines = open(data_path).read().split('\\n')\n",
    "    with open(name_to_save + \"_src-\" + mode +\".txt\", \"w\") as f1, open(name_to_save + \"_tgt-\" + mode + \".txt\", \"w\") as f2:\n",
    "        for line in lines[3: len(lines) - 1]:\n",
    "            input_text, target_text, *c = line.split('\\t')\n",
    "            input_text,_ = input_text.split('>')\n",
    "            input_text = input_text.split()\n",
    "            target_text = target_text.split()\n",
    "            f1.write(\" \".join(input_text) + \" \\n\")\n",
    "            f2.write(\" \".join(target_text) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/US_patents_1976-Sep2016_1product_reactions_train.csv\"\n",
    "name_to_save = \"data/USP\"\n",
    "mode = \"train\"\n",
    "separate_data(data_path, name_to_save, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/US_patents_1976-Sep2016_1product_reactions_valid.csv\"\n",
    "name_to_save = \"data/USP\"\n",
    "mode = \"valid\"\n",
    "separate_data(data_path, name_to_save, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = PreprocessorOpts(train_src=name_to_save + \"_src-train.txt\", \n",
    "                  train_tgt=name_to_save + \"_tgt-train.txt\", \n",
    "                  valid_src=name_to_save + \"_src-valid.txt\",\n",
    "                  valid_tgt=name_to_save + \"_tgt-valid.txt\",\n",
    "                  save_data=\"data/USP\")\n",
    "src_nfeats = inputters.get_num_features(\n",
    "    opt.data_type, opt.train_src, 'src')\n",
    "tgt_nfeats = inputters.get_num_features(\n",
    "    opt.data_type, opt.train_tgt, 'tgt')\n",
    "fields = inputters.get_fields(opt.data_type, src_nfeats, tgt_nfeats)\n",
    "train_dataset_files = build_save_dataset('train', fields, opt)\n",
    "build_save_dataset('valid', fields, opt)\n",
    "build_save_vocab(train_dataset_files, fields, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-11-14 23:56:51,514 INFO] Loading train dataset from data\\USP.train.0.pt, number of examples: 189902\n",
      "[2018-11-14 23:56:51,522 INFO]  * vocabulary size. source = 178; target = 101\n",
      "[2018-11-14 23:56:51,524 INFO] Building model...\n",
      "[2018-11-14 23:56:51,690 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(178, 500, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(500, 500, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(101, 500, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(1000, 500)\n",
      "        (1): LSTMCell(500, 500)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (linear_out): Linear(in_features=1000, out_features=500, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=500, out_features=101, bias=True)\n",
      "    (1): LogSoftmax()\n",
      "  )\n",
      ")\n",
      "[2018-11-14 23:56:51,693 INFO] encoder: 4097000\n",
      "[2018-11-14 23:56:51,695 INFO] decoder: 5859101\n",
      "[2018-11-14 23:56:51,698 INFO] * number of parameters: 9956101\n",
      "[2018-11-14 23:56:51,701 INFO] Start training...\n",
      "[2018-11-14 23:56:53,120 INFO] Loading train dataset from data\\USP.train.0.pt, number of examples: 189902\n",
      "[2018-11-14 23:59:52,327 INFO] Step 50/   50; acc:  19.12; ppl: 274.71; xent: 5.62; lr: 1.00000; 598/563 tok/s;    179 sec\n",
      "[2018-11-14 23:59:54,369 INFO] Loading train dataset from data\\USP.train.0.pt, number of examples: 189902\n"
     ]
    }
   ],
   "source": [
    "opt = Opts(data=\"data/USP\", save_model=\"demo_model\", train_steps=50, epochs=1)\n",
    "single_main(opt, -1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
